\renewcommand{\baselinestretch}{1.5}\normalsize

\chapter{Introduction}

%\thispagestyle{empty}

%\begin{refsection} % Starts a new reference section
	
\label{Chapter Introduction}

\section{Introduction to Deep Learning}
 \cite{goodfellow2014generative} \cite{lecun2015deep}
Deep learning is a subfield of machine learning that uses artificial neural networks (ANNs) with multiple layers to model complex patterns in data. Inspired by the structure and function of the human brain, deep learning has revolutionized fields such as computer vision, natural language processing (NLP), speech recognition, and robotics.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.35\linewidth]{./graphics/raitlogo.jpeg} % 
	\hspace{2cm}
	\caption  {Logo}
	
\end{figure}

\subsection{Core Concepts in Deep Learning}
\subsubsection{Neural Networks}
\begin{itemize}
	\item \textbf{Artificial Neuron}: A computational unit that takes inputs, applies weights, adds a bias, and passes the result through an activation function.
	\item \textbf{Layers}:
	\begin{itemize}
		\item \textbf{Input Layer}: Receives raw data (e.g., pixels in an image)
		\item \textbf{Hidden Layers}: Intermediate layers that extract features
		\item \textbf{Output Layer}: Produces predictions (e.g., class probabilities)
	\end{itemize}
\end{itemize}

\subsubsection{Activation Functions}
Introduce non-linearity to enable learning complex patterns.

Common functions:
\begin{itemize}
	\item \textbf{ReLU (Rectified Linear Unit)}: $f(x) = \max(0, x)$ (most widely used)
	\item \textbf{Sigmoid}: $f(x) = \frac{1}{1 + e^{-x}}$ (for binary classification)
	\item \textbf{Tanh}: Scaled version of sigmoid, output between -1 and 1
	\item \textbf{Softmax}: Used in multi-class classification to output probabilities
\end{itemize}

\subsubsection{Loss Functions}
Measure how well the model's predictions match the true labels.

Examples:
\begin{itemize}
	\item \textbf{Mean Squared Error (MSE)}: For regression tasks
	\item \textbf{Cross-Entropy Loss}: For classification tasks
\end{itemize}

\subsubsection{Optimization Algorithms}
Adjust weights to minimize loss.
\begin{itemize}
	\item \textbf{Stochastic Gradient Descent (SGD)}: Basic optimizer
	\item \textbf{Adam}: Adaptive learning rate optimizer (widely used)
	\item \textbf{RMSprop}: Optimizer for recurrent networks
\end{itemize}

\subsection{Types of Deep Learning Architectures}
\subsubsection{Feedforward Neural Networks (FNNs)}
\begin{itemize}
	\item Simplest form, data flows in one direction (input $\rightarrow$ hidden $\rightarrow$ output)
	\item Used for structured data (e.g., tabular data)
\end{itemize}

\subsubsection{Convolutional Neural Networks (CNNs)}
Specialized for grid-like data (images, videos).

Key components:
\begin{itemize}
	\item \textbf{Convolutional Layers}: Apply filters to detect features (edges, textures)
	\item \textbf{Pooling Layers}: Reduce spatial dimensions (max pooling, average pooling)
	\item \textbf{Fully Connected Layers}: Final classification
\end{itemize}

Applications: Image classification, object detection (YOLO, Faster R-CNN).

\subsubsection{Recurrent Neural Networks (RNNs)}
Designed for sequential data (text, time series).
\begin{itemize}
	\item \textbf{Long Short-Term Memory (LSTM)}: Handles long-term dependencies
	\item \textbf{Gated Recurrent Unit (GRU)}: Simpler than LSTM, faster training
\end{itemize}

Applications: Machine translation, speech recognition.

\subsubsection{Transformers \cite{vaswani2017attention}}
Uses \textbf{self-attention} to process sequences in parallel (unlike RNNs).
\begin{itemize}
	\item \textbf{BERT, GPT}: State-of-the-art NLP models
\end{itemize}

Applications: Chatbots, text summarization.

\subsubsection{Generative Models}
\begin{itemize}
	\item \textbf{Generative Adversarial Networks (GANs)}: Generator vs. discriminator (e.g., Deepfake, image synthesis)
	\item \textbf{Variational Autoencoders (VAEs)}: Learn latent representations for generation
\end{itemize}



\section{Motivation}

dasdadadada


\section{Applications}

dasdadadsa

\begin{itemize}
	
	\item \textbf{AAAAA}
	
	jdahdjkhajsdhjakhdjka
	
\end{itemize}


\section{Organization of the report}

The report is organized as follows: 

Chapter 2: Talks about existing literatures review. 
